{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "np.random.seed(2)\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4*T, 4*T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 1)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = Variable(torch.zeros(input.size(0), 51).double(), requires_grad=False)\n",
    "        c_t = Variable(torch.zeros(input.size(0), 51).double(), requires_grad=False)\n",
    "        h_t2 = Variable(torch.zeros(input.size(0), 1).double(), requires_grad=False)\n",
    "        c_t2 = Variable(torch.zeros(input.size(0), 1).double(), requires_grad=False)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))\n",
    "            outputs += [c_t2]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(c_t2, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(c_t, (h_t2, c_t2))\n",
    "            outputs += [c_t2]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0\n",
      "loss: 1.09054480685\n",
      "loss: 0.826498405052\n",
      "loss: 0.742258946371\n",
      "loss: 0.4669266811\n",
      "loss: 0.331289496509\n",
      "loss: 0.236026195159\n",
      "loss: 0.141259129094\n",
      "loss: 0.0851530448202\n",
      "loss: 0.0541042162156\n",
      "loss: 0.0400308146904\n",
      "loss: 0.0348454014688\n",
      "loss: 0.0298759461389\n",
      "loss: 0.0270200258218\n",
      "loss: 0.0249405790248\n",
      "loss: 0.0236001234783\n",
      "loss: 0.0191040909446\n",
      "loss: 0.0133801821148\n",
      "loss: 0.0113662791065\n",
      "loss: 0.00983567492028\n",
      "loss: 0.00893714785276\n",
      "STEP:  1\n",
      "loss: 0.0085707915367\n",
      "loss: 0.00803035282722\n",
      "loss: 0.0071075706583\n",
      "loss: 0.00923094992072\n",
      "loss: 0.00576149139876\n",
      "loss: 0.00500355377519\n",
      "loss: 0.0213273585187\n",
      "loss: 0.00430447187568\n",
      "loss: 0.00387833926829\n",
      "loss: 0.00434628558957\n",
      "loss: 0.00298486789118\n",
      "loss: 0.00266434974742\n",
      "loss: 0.00258906901548\n",
      "loss: 0.00245833529464\n",
      "loss: 0.00244060736007\n",
      "loss: 0.0023920635679\n",
      "loss: 0.00233781562643\n",
      "loss: 0.00224966389398\n",
      "loss: 0.00196080221474\n",
      "loss: 0.0345017859766\n",
      "STEP:  2\n",
      "loss: 0.00191543395452\n",
      "loss: 0.00190125225438\n",
      "loss: 0.00187472642341\n",
      "loss: 0.00185644249509\n",
      "loss: 0.00180629461682\n",
      "loss: 0.00177696366129\n",
      "loss: 0.00174659504421\n",
      "loss: 0.0017387482606\n",
      "loss: 0.00182761049761\n",
      "loss: 0.00166137954403\n",
      "loss: 0.0016097974455\n",
      "loss: 0.00147308448323\n",
      "loss: 0.0014492781253\n",
      "loss: 0.00140032900178\n",
      "loss: 0.00139098706182\n",
      "loss: 0.00138298225904\n",
      "loss: 0.00136611763681\n",
      "loss: 0.0013293909144\n",
      "loss: 0.00147490851951\n",
      "loss: 0.00129273110467\n",
      "STEP:  3\n",
      "loss: 0.00126987656359\n",
      "loss: 0.00128890989406\n",
      "loss: 0.00123731048507\n",
      "loss: 0.00121360034484\n",
      "loss: 857.012826312\n",
      "loss: 0.00120888468318\n",
      "loss: 0.0012389643507\n",
      "loss: 0.00119208933503\n",
      "loss: 0.00118458473761\n",
      "loss: 0.00117354575669\n",
      "loss: 0.00117841215386\n",
      "loss: 0.00108085738621\n",
      "loss: 0.00100656837934\n",
      "loss: 0.0942499664321\n",
      "loss: 0.000998976893537\n",
      "loss: 0.00099494782858\n",
      "loss: 0.000945828737902\n",
      "loss: 0.000889498565633\n",
      "loss: 0.00228605028581\n",
      "loss: 0.000829880891213\n",
      "STEP:  4\n",
      "loss: 0.000783744197417\n",
      "loss: 0.000725891627299\n",
      "loss: 0.00295195757787\n",
      "loss: 0.000688624097653\n",
      "loss: 0.000658446380953\n",
      "loss: 0.000624062828297\n",
      "loss: 0.000494142848724\n",
      "loss: 0.000436191832831\n",
      "loss: 0.000454812067308\n",
      "loss: 0.000395661919197\n",
      "loss: 0.000386490248013\n",
      "loss: 0.000380677633375\n",
      "loss: 0.000378493890158\n",
      "loss: 0.000353145617721\n",
      "loss: 0.000313462851076\n",
      "loss: 0.000295287991548\n",
      "loss: 0.000269255371128\n",
      "loss: 0.000241959787007\n",
      "loss: 0.000229621471103\n",
      "loss: 0.000217402156819\n",
      "STEP:  5\n",
      "loss: 0.000194745214123\n",
      "loss: 0.000179156849642\n",
      "loss: 0.000173176786347\n",
      "loss: 0.000173010594749\n",
      "loss: 0.000167423082083\n",
      "loss: 0.000167729055986\n",
      "loss: 0.000166351668727\n",
      "loss: 0.000166203305806\n",
      "loss: 0.000164829560114\n",
      "loss: 0.00016343480816\n",
      "loss: 0.000159279129264\n",
      "loss: 0.000151235877929\n",
      "loss: 0.000137710788027\n",
      "loss: 0.000127196380051\n",
      "loss: 0.000204914579098\n",
      "loss: 0.000129983793292\n",
      "loss: 0.000186934145986\n",
      "loss: 0.000104662788322\n",
      "loss: 9.71538937749e-05\n",
      "loss: 9.44022320324e-05\n",
      "STEP:  6\n",
      "loss: 8.90554721871e-05\n",
      "loss: 8.58639661224e-05\n",
      "loss: 8.59501908403e-05\n",
      "loss: 8.06850583472e-05\n",
      "loss: 7.85517638958e-05\n",
      "loss: 8.85480964343e-05\n",
      "loss: 7.5607746042e-05\n",
      "loss: 7.39671353832e-05\n",
      "loss: 0.000117278598183\n",
      "loss: 7.27699806359e-05\n",
      "loss: 7.19007344291e-05\n",
      "loss: 0.000117520853741\n",
      "loss: 7.10520321506e-05\n",
      "loss: 7.03806662896e-05\n",
      "loss: 9.35785210412e-05\n",
      "loss: 6.95723808178e-05\n",
      "loss: 6.90009503552e-05\n",
      "loss: 7.0333614714e-05\n",
      "loss: 6.8162398264e-05\n",
      "loss: 6.79023934121e-05\n",
      "STEP:  7\n",
      "loss: 6.77334435636e-05\n",
      "loss: 6.76516150783e-05\n",
      "loss: 6.74286434393e-05\n",
      "loss: 6.73782871006e-05\n",
      "loss: 6.72791036789e-05\n",
      "loss: 6.67257623426e-05\n",
      "loss: 6.60945581338e-05\n",
      "loss: 6.51970919987e-05\n",
      "loss: 6.4358776143e-05\n",
      "loss: 6.3588050418e-05\n",
      "loss: 6.29138287668e-05\n",
      "loss: 6.24882409901e-05\n",
      "loss: 6.21198954903e-05\n",
      "loss: 6.18149806333e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-79f5dfaac835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# begin to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/torch/optim/lbfgs.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# the reason we do this: in a stochastic setting,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mabs_grad_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-79f5dfaac835>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/torch/autograd/_functions/pointwise.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input = Variable(torch.from_numpy(data[3:, :-1]), requires_grad=False)\n",
    "target = Variable(torch.from_numpy(data[3:, 1:]), requires_grad=False)\n",
    "\n",
    "seq = Sequence()\n",
    "seq.double() # all parameters and buffers\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(seq.parameters()) #?\n",
    "\n",
    "for i in range(15):\n",
    "    print('STEP: ', i)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('loss:', loss.data.numpy()[0])\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "    # begin to predict\n",
    "    future = 1000\n",
    "    pred = seq(input[:3], future = future)\n",
    "    y = pred.data.numpy()\n",
    "    # draw the result\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30) \n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "        plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "    draw(y[0], 'r')\n",
    "    draw(y[1], 'g')\n",
    "    draw(y[2], 'b')\n",
    "    plt.savefig('predict%d.pdf'%i)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
